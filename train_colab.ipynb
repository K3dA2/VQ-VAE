{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPjtZjSLTtjUXqbSFVVXgXW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/K3dA2/VQ-VAE/blob/main/train_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "66ZpCzGDFpNO"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install kaggle\n",
        "! mkdir ~/.kaggle"
      ],
      "metadata": {
        "id": "U4dq6F4sFp-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/drive/MyDrive/kaggle.json ~/.kaggle/kaggle.json\n",
        "! kaggle datasets download scribbless/another-anime-face-dataset\n",
        "! unzip another-anime-face-dataset.zip"
      ],
      "metadata": {
        "id": "6ozkC0dAFspu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/K3dA2/VQ-VAE.git"
      ],
      "metadata": {
        "id": "7DpAfBToFvEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/VQ-VAE/')"
      ],
      "metadata": {
        "id": "SB0Z1CfxGdkY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "import datetime\n",
        "import os\n",
        "import torch.nn.utils as utils\n",
        "from model import VQVAE\n",
        "from utils import get_data_loader,count_parameters\n",
        "import uuid"
      ],
      "metadata": {
        "id": "A4RIinyzGlF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def training_loop(n_epochs, optimizer, model, loss_fn, device, data_loader,\\\n",
        "                   max_grad_norm=1.0, epoch_start = 0,\\\n",
        "                    save_img = False, show_img = True):\n",
        "    model.train()\n",
        "    for epoch in range(epoch_start,n_epochs):\n",
        "        loss_train = 0.0\n",
        "\n",
        "        progress_bar = tqdm(data_loader, desc=f'Epoch {epoch}', unit=' batch')\n",
        "        for imgs, _ in progress_bar:\n",
        "            imgs = imgs.to(device)\n",
        "\n",
        "\n",
        "            outputs,c_loss = model(imgs)\n",
        "            loss = loss_fn(outputs, imgs) + c_loss\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            #utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "            optimizer.step()\n",
        "\n",
        "            loss_train += loss.item()\n",
        "            progress_bar.set_postfix(loss=loss.item())\n",
        "\n",
        "        # Save model checkpoint with the current epoch in the filename\n",
        "        model_filename = f'waifu-vqvae.pth'\n",
        "        model_path = os.path.join('/content/drive/MyDrive/VQ-VAE Weights/', model_filename)\n",
        "\n",
        "        with open(\"waifu-vqvae_epoch-loss.txt\", \"a\") as file:\n",
        "            file.write(f\"{loss_train / len(data_loader)}\\n\")\n",
        "\n",
        "        print('{} Epoch {}, Training loss {}'.format(datetime.datetime.now(), epoch, loss_train / len(data_loader)))\n",
        "        if epoch % 20 == 0:\n",
        "            if show_img:\n",
        "                pred_images = model.inference(1, 14, 14)\n",
        "                plt.imshow(np.transpose(pred_images[-1].cpu().numpy(), (1, 2, 0)))\n",
        "                plt.show()\n",
        "            if save_img:\n",
        "                pred_images = model.inference(1, 14, 14)\n",
        "                pred_images = np.transpose(pred_images[-1].cpu().numpy(), (1, 2, 0))\n",
        "                random_filename = str(uuid.uuid4()) + '.png'\n",
        "\n",
        "                # Specify the directory where you want to save the image\n",
        "                save_directory = path\n",
        "\n",
        "                # Create the full path including the directory and filename\n",
        "                full_path = os.path.join(save_directory, random_filename)\n",
        "                # Save the image with the random filename\n",
        "                plt.savefig(full_path, bbox_inches='tight', pad_inches=0)\n",
        "\n",
        "            torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            }, model_path)"
      ],
      "metadata": {
        "id": "jyFw3qzqGoaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/animefaces256cleaner'\n",
        "#model_path = '/content/drive/MyDrive/VQ-VAE Weights/waifu-vqvae.pth'\n",
        "\n",
        "device = \"cpu\"\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
        "    device = \"mps\"\n",
        "print(f\"using device: {device}\")\n",
        "\n",
        "model = VQVAE()  # Assuming Unet is correctly imported and defined\n",
        "model.to(device)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-4)\n",
        "#loss_fn = nn.L1Loss().to(device)\n",
        "loss_fn = nn.MSELoss().to(device)\n",
        "print(count_parameters(model))\n",
        "data_loader = get_data_loader(path, batch_size=16,num_samples=24_000)"
      ],
      "metadata": {
        "id": "diqiriP8GrFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optionally load model weights if needed\n",
        "#checkpoint = torch.load(model_path)\n",
        "#model.load_state_dict(checkpoint['model_state_dict'])\n",
        "#optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "#epoch = checkpoint['epoch']"
      ],
      "metadata": {
        "id": "6SHFeqIcHbmc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_loop(\n",
        "    n_epochs=1000,\n",
        "    optimizer=optimizer,\n",
        "    model=model,\n",
        "    loss_fn=loss_fn,\n",
        "    device=device,\n",
        "    data_loader=data_loader,\n",
        "    epoch_start= 0,\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "2KP57RxsHeVM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}