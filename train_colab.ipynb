{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/K3dA2/VQ-VAE/blob/main/train_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66ZpCzGDFpNO",
        "outputId": "85ea678a-e097-4a94-8a7b-cc22b3d90856"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U4dq6F4sFp-P",
        "outputId": "d82814d5-d82e-411b-a16b-7f5a0050771f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.6.14)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2024.7.4)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.4)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.0.7)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.7)\n",
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n"
          ]
        }
      ],
      "source": [
        "! pip install kaggle\n",
        "! mkdir ~/.kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ozkC0dAFspu",
        "outputId": "714ceea3-04e9-4560-c788-e9518e68fe7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/scribbless/another-anime-face-dataset\n",
            "License(s): GPL-2.0\n",
            "another-anime-face-dataset.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "Archive:  another-anime-face-dataset.zip\n",
            "replace animefaces256cleaner/10004131_result.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ],
      "source": [
        "!cp /content/drive/MyDrive/kaggle.json ~/.kaggle/kaggle.json\n",
        "! kaggle datasets download scribbless/another-anime-face-dataset\n",
        "! unzip another-anime-face-dataset.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7DpAfBToFvEd",
        "outputId": "10f12dba-fd64-42f4-e106-d641ab7a1fef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'VQ-VAE' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/K3dA2/VQ-VAE.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "SB0Z1CfxGdkY"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('/content/VQ-VAE/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4RIinyzGlF2",
        "outputId": "ced5632f-40bf-4eac-bc3b-341eda74b694"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "out shape: torch.Size([2, 3, 128, 128])\n",
            "loss shape: 0.5168724656105042\n",
            "torch.Size([1, 3, 64, 64])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "import datetime\n",
        "import os\n",
        "import torch.nn.utils as utils\n",
        "from model import VQVAE\n",
        "from utils import get_data_loader,count_parameters\n",
        "import uuid\n",
        "import os\n",
        "import random\n",
        "from PIL import Image\n",
        "from torch.utils.data import DataLoader, Dataset, Subset\n",
        "from torchvision import transforms\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "0-ZKoOdIKNh4"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.image_paths = [os.path.join(root_dir, fname) for fname in os.listdir(root_dir) if os.path.isfile(os.path.join(root_dir, fname))]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # Return the image and a dummy label (e.g., 0)\n",
        "        return image, 0\n",
        "\n",
        "def get_data_loader(path, batch_size, num_samples=None, shuffle=True):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((64, 64)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.7002, 0.6099, 0.6036), (0.2195, 0.2234, 0.2097))\n",
        "    ])\n",
        "\n",
        "    full_dataset = CustomDataset(root_dir=path, transform=transform)\n",
        "\n",
        "    if num_samples is None or num_samples > len(full_dataset):\n",
        "        num_samples = len(full_dataset)\n",
        "    print(\"data length:\", len(full_dataset))\n",
        "\n",
        "    if shuffle:\n",
        "        indices = random.sample(range(len(full_dataset)), num_samples)\n",
        "    else:\n",
        "        indices = list(range(num_samples))\n",
        "\n",
        "    subset_dataset = Subset(full_dataset, indices)\n",
        "\n",
        "    data_loader = DataLoader(subset_dataset, batch_size=batch_size, shuffle=shuffle)\n",
        "\n",
        "    return data_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RjAtVvVWKPNE",
        "outputId": "b116a0fc-0b2a-4d50-82ad-36fbe8eec7b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data length: 92219\n"
          ]
        }
      ],
      "source": [
        "path = '/content/animefaces256cleaner'\n",
        "batch_size = 32\n",
        "data_loader = get_data_loader(path, batch_size, num_samples=None, shuffle=True)\n",
        "writer = SummaryWriter(log_dir='/content/drive/MyDrive/VQ-VAE Weights/logs')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "jyFw3qzqGoaL"
      },
      "outputs": [],
      "source": [
        "def training_loop(n_epochs, optimizer, model, loss_fn, device, data_loader,\\\n",
        "                   max_grad_norm=1.0, epoch_start = 0,\\\n",
        "                    save_img = False, show_img = True):\n",
        "    model.train()\n",
        "    for epoch in range(epoch_start,n_epochs):\n",
        "        loss_train = 0.0\n",
        "\n",
        "        progress_bar = tqdm(data_loader, desc=f'Epoch {epoch}', unit=' batch')\n",
        "        for batch_idx, (imgs, _) in enumerate(progress_bar):\n",
        "            imgs = imgs.to(device)\n",
        "\n",
        "\n",
        "            outputs,vq_loss = model(imgs)\n",
        "            mse_loss = loss_fn(outputs, imgs)\n",
        "            loss = mse_loss + vq_loss \n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            #utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "            optimizer.step()\n",
        "\n",
        "            loss_train += loss.item()\n",
        "\n",
        "            # Log losses to TensorBoard\n",
        "            writer.add_scalar('Loss/Total', loss.item(), epoch * len(data_loader) + batch_idx)\n",
        "            writer.add_scalar('Loss/Reconstruction', mse_loss.item(), epoch * len(data_loader) + batch_idx)\n",
        "            writer.add_scalar('Loss/VectorQuantization', vq_loss.item(), epoch * len(data_loader) + batch_idx)\n",
        "\n",
        "            # Log embeddings to TensorBoard\n",
        "            if batch_idx % 10 == 0:\n",
        "                writer.add_embedding(\n",
        "                    model.codebook.weight.data,\n",
        "                    metadata=[f\"embedding_{i}\" for i in range(model.num_embeddings)],\n",
        "                    global_step=epoch * len(data_loader) + batch_idx,\n",
        "                    tag='Codebook'\n",
        "                )\n",
        "\n",
        "            progress_bar.set_postfix(loss=loss.item())\n",
        "\n",
        "        # Save model checkpoint with the current epoch in the filename\n",
        "        model_filename = f'waifu-vqvae.pth'\n",
        "        model_path = os.path.join('/content/drive/MyDrive/VQ-VAE Weights/', model_filename)\n",
        "\n",
        "        with open(\"waifu-vqvae_epoch-loss.txt\", \"a\") as file:\n",
        "            file.write(f\"{loss_train / len(data_loader)}\\n\")\n",
        "\n",
        "        print('{} Epoch {}, Training loss {}'.format(datetime.datetime.now(), epoch, loss_train / len(data_loader)))\n",
        "        if epoch % 20 == 0:\n",
        "            if show_img:\n",
        "                pred_images = model.inference(1, 14, 14)\n",
        "                plt.imshow(np.transpose(pred_images[-1].cpu().numpy(), (1, 2, 0)))\n",
        "                plt.show()\n",
        "            if save_img:\n",
        "                pred_images = model.inference(1, 14, 14)\n",
        "                pred_images = np.transpose(pred_images[-1].cpu().numpy(), (1, 2, 0))\n",
        "                random_filename = str(uuid.uuid4()) + '.png'\n",
        "\n",
        "                # Specify the directory where you want to save the image\n",
        "                save_directory = path\n",
        "\n",
        "                # Create the full path including the directory and filename\n",
        "                full_path = os.path.join(save_directory, random_filename)\n",
        "                # Save the image with the random filename\n",
        "                plt.savefig(full_path, bbox_inches='tight', pad_inches=0)\n",
        "\n",
        "            torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            }, model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "diqiriP8GrFb",
        "outputId": "07c32449-1238-4262-854f-76a3be0d7b9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "using device: cpu\n",
            "31209996\n"
          ]
        }
      ],
      "source": [
        "path = '/content/animefaces256cleaner'\n",
        "#model_path = '/content/drive/MyDrive/VQ-VAE Weights/waifu-vqvae.pth'\n",
        "\n",
        "device = \"cpu\"\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
        "    device = \"mps\"\n",
        "print(f\"using device: {device}\")\n",
        "\n",
        "model = VQVAE()  # Assuming Unet is correctly imported and defined\n",
        "model.to(device)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-4)\n",
        "#loss_fn = nn.L1Loss().to(device)\n",
        "loss_fn = nn.MSELoss().to(device)\n",
        "print(count_parameters(model))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "6SHFeqIcHbmc"
      },
      "outputs": [],
      "source": [
        "# Optionally load model weights if needed\n",
        "#checkpoint = torch.load(model_path)\n",
        "#model.load_state_dict(checkpoint['model_state_dict'])\n",
        "#optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "#epoch = checkpoint['epoch']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2KP57RxsHeVM",
        "outputId": "a4551a3e-7274-41e1-a250-fff32b00b194"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpoch 0:   0%|          | 0/2882 [00:00<?, ? batch/s]"
          ]
        }
      ],
      "source": [
        "training_loop(\n",
        "    n_epochs=1000,\n",
        "    optimizer=optimizer,\n",
        "    model=model,\n",
        "    loss_fn=loss_fn,\n",
        "    device=device,\n",
        "    data_loader=data_loader,\n",
        "    epoch_start= 0,\n",
        "\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyOusACWYpBeBZ59gIkwZBbx",
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
